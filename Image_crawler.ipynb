{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pf16OWrxcR"
      },
      "source": [
        "<h1>Image crawler</h1>\n",
        "<b>Introduction</b><br/>\n",
        "<div>In this notebook, we are going to find pictures of Anne Shirley's quotes by crawling the web, extract the text of the pictures and translate them into Persian, then paste the translated text on the new pictures. We do this process by searching the web using image crawlers. Many tools can be found for this task provided by various developers, whatever we use in this notebook also mention its source. We do this in the following steps.</div>\n",
        "\n",
        "<div>A Web crawler, sometimes called a <b>spider</b> or <b>spiderbot</b> and often shortened to <b>crawler</b>, is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing (web spidering).<a href=\"https://en.wikipedia.org/wiki/Web_crawler#cite_note-1\">[1]</a></div><br/>\n",
        "<div>The image crawler application is used to collect a multitude of images from websites. The images can be viewed as thumbnails or saved to a given folder for enhanced processing.<a href=\"https://sourceforge.net/projects/imagecrawler/\">[2]</a><br/>\n",
        "</div>\n",
        "\n",
        "<b>Contents</b><br/>\n",
        "<ul>\n",
        "  <li>Installations</li>\n",
        "      <ul>\n",
        "          <li>icrawler</li>\n",
        "          <li>easyocr</li>\n",
        "          <li>pyspellchecker</li>\n",
        "          <li>fuzzywuzzy</li>\n",
        "          <li>deep-translator</li>\n",
        "      </ul>\n",
        "  <li>Import libraries</li>\n",
        "  <li>Implementation helper functions</li>\n",
        "  <li>Main body</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEHuSwuzrxcX"
      },
      "source": [
        "<h1><b>Installations</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iyqZC04rxcX"
      },
      "source": [
        "<b>Install icrawler</b><br/>\n",
        "In this project, we use the icrawler library, if you haven't installed it yet, you should install it(Python 3.5+ recommended).<br/>\n",
        "With this package, you can write a multiple thread crawler easily by focusing on the contents you want to crawl, keeping away from troublesome problems like exception handling, thread scheduling and communication.<a href='https://pypi.org/project/icrawler/'>[3]</a>\n",
        "\n",
        "It also provides built-in crawlers for popular image sites like Flickr and search engines such as Google, Bing and Baidu.<a href='https://pypi.org/project/icrawler/'>[4]</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC_G76d4rxcY",
        "outputId": "e92ca0ae-ad37-40d1-cada-5f056be5cf3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.6-py2.py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from icrawler) (4.9.2)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.9/dist-packages (from icrawler) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from icrawler) (4.11.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from icrawler) (8.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from icrawler) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.4.1->icrawler) (2.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->icrawler) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->icrawler) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->icrawler) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->icrawler) (1.26.15)\n",
            "Installing collected packages: icrawler\n",
            "Successfully installed icrawler-0.6.6\n"
          ]
        }
      ],
      "source": [
        "# if you haven't installed it yet, you should install it with the following command.\n",
        "# Run this line just first time\n",
        "%pip install icrawler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6YSeWYGk81d"
      },
      "source": [
        "<h1>easyocr</h1>\n",
        "EasyOCR is a Python package that provides a ready-to-use OCR engine and supports 80+ languages. EasyOCR is easy to install and very straightforward to use. This makes it a great solution for performing OCR with Python. You only need to install the PyTorch (required on Windows only) and EasyOCR packages, and then you can start extracting text from images using Python.\n",
        "\n",
        "To use EasyOCR on Windows, you must install the PyTorch and EasyOCR packages. Run the following commands in sequence to install the packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyjUmMvOaenk",
        "outputId": "695ce5ff-869b-4686-eaa3-fa7728f4fb4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.6.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.9/dist-packages (from easyocr) (0.14.1+cu116)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from easyocr) (6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from easyocr) (8.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from easyocr) (0.19.3)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.9/dist-packages (from easyocr) (2.0.1)\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from easyocr) (1.13.1+cu116)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from easyocr) (1.10.1)\n",
            "Collecting pyclipper\n",
            "  Downloading pyclipper-1.3.0.post4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (608 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.2/608.2 KB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from easyocr) (1.22.4)\n",
            "Collecting opencv-python-headless<=4.5.4.60\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5->easyocr) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5->easyocr) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from python-bidi->easyocr) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image->easyocr) (3.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->easyocr) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image->easyocr) (23.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->easyocr) (2023.3.21)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->easyocr) (1.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5->easyocr) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5->easyocr) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5->easyocr) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5->easyocr) (2022.12.7)\n",
            "Installing collected packages: pyclipper, ninja, python-bidi, opencv-python-headless, easyocr\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.7.0.72\n",
            "    Uninstalling opencv-python-headless-4.7.0.72:\n",
            "      Successfully uninstalled opencv-python-headless-4.7.0.72\n",
            "Successfully installed easyocr-1.6.2 ninja-1.11.1 opencv-python-headless-4.5.4.60 pyclipper-1.3.0.post4 python-bidi-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio\n",
        "%pip install easyocr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEKYj1cjrxca"
      },
      "source": [
        "<b>pyspellchecker</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7pFAA5vrxcb",
        "outputId": "31ed9aed-3423-4be4-827b-d78626324df4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.1\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AiTt13mrxcb"
      },
      "source": [
        "<b>fuzzywuzzy</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJmpUP-1rxcb",
        "outputId": "09eda524-59da-4fe6-8b2d-8d15bf2c775a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.20.9\n",
            "  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.20.9 python-Levenshtein-0.20.9 rapidfuzz-2.13.7\n"
          ]
        }
      ],
      "source": [
        "%pip install fuzzywuzzy\n",
        "%pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85ZZHw0nrxcc"
      },
      "source": [
        "<b>deep-translator</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4klXNYRrxcc",
        "outputId": "40d9fba4-5860-47fe-f32f-f17040cbc728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.10.1-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from deep-translator) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from deep-translator) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.15)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.10.1\n"
          ]
        }
      ],
      "source": [
        "%pip install deep-translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwHbmglLrxcd"
      },
      "source": [
        "<h1><b>Import libraries</b></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F6nxx8ovrxcd"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import sys\n",
        "import os,shutil\n",
        "import re\n",
        "#Options for spell cheking\n",
        "# %pip uninstall PyEnchant\n",
        "# %pip install textblo\n",
        "# %pip install jamspell\n",
        "# %pip install autocorrect\n",
        "from spellchecker import SpellChecker\n",
        "from fuzzywuzzy import fuzz\n",
        "from icrawler.builtin import bing\n",
        "from icrawler.builtin import google \n",
        "#Other options\n",
        "# from icrawler.builtin import baidu\n",
        "# from icrawler.builtin import flickr\n",
        "\n",
        "from typing import Literal\n",
        "#import arabic_reshaper\n",
        "#from bidi.algorithm import get_display\n",
        "from string import ascii_letters\n",
        "import textwrap\n",
        "from PIL import ImageFont\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from deep_translator import GoogleTranslator\n",
        "#import pytesseract(replaced by EasyOCR)\n",
        "import easyocr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBR1TqHirxcd"
      },
      "source": [
        "<h1>Initailize</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-11pz5MGrxce",
        "outputId": "7f18c625-9f49-4750-bbe5-98acfde023a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:easyocr.easyocr:Using CPU. Note: This module is much faster with a GPU.\n"
          ]
        }
      ],
      "source": [
        "workspace = \"\"\n",
        "root_directory =\"\"\n",
        "quote_path = \"\"\n",
        "output_path=\"\"\n",
        "wallpaper_keyword = \"sunny\"\n",
        "anne_keyword=\"AnneShirleyQuotes\"\n",
        "# The maximum number for Crawling image from web\n",
        "max_number = 35\n",
        "# fuzzy text matching threshold(detected text and anne book content)\n",
        "threshold = 90\n",
        "\n",
        "translator = GoogleTranslator(source='auto', target='fa')\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Replaced by easyocr tools\n",
        "#pytesseract.pytesseract.tesseract_cmd =(r'/usr/bin/tesseract')\n",
        "#Whe local: 'C:/Program Files/Tesseract-OCR/tesseract.exe' \n",
        "\n",
        "lang_list = ['en']\n",
        "#You can pass several languages at once but not all languages can be used together. \n",
        "# English is compatible with every language and languages that share common \n",
        "# characters are usually compatible with each other.\n",
        "\n",
        "# The line is for loading a model into memory. It takes some time but it needs to be run only once.\n",
        "easyocr_reader = easyocr.Reader(lang_list,gpu=False)\n",
        "# In case you do not have a GPU, or your GPU has low memory, you can run the model in CPU-only mode by adding gpu=False.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Dol8J7rxce"
      },
      "source": [
        "<h1><b>Implementation helper functions</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDQmL5Berxce"
      },
      "source": [
        "Implementation of get_crawled_image_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VHqtKmkXrxce"
      },
      "outputs": [],
      "source": [
        "# returns crawled images directory and list of images in\n",
        "def get_crawled_image_names(parent_path=None):\n",
        "    # get list of crawled images\n",
        "    # Note: crawled imaged by icrawler is stored in a folder named as 'Images'\n",
        "    # in root directory\n",
        "    list_ = os.listdir(parent_path)\n",
        "    # filter files in the 'Images' folder by valid extensions\n",
        "    ext = ['jpg','jpeg','png','bmp']\n",
        "\n",
        "    # Filter file names based on given extensions in list 2\n",
        "    filtered = [x for x in list_ if any(y in x for y in ext)]\n",
        "    \n",
        "    # Returns crawled image names as list\n",
        "    return filtered\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVTwesA2rxcf"
      },
      "source": [
        "Implementation of text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3TrNebQdrxcf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_text(text=''):\n",
        "  # re.IGNORECASE ignoring cases\n",
        "  # compilation step to escape the word for all cases\n",
        "  compiled = re.compile(re.escape('anne'), re.IGNORECASE)\n",
        "  text = compiled.sub('', text)\n",
        "  compiled = re.compile(re.escape('shirley'), re.IGNORECASE)\n",
        "  text = compiled.sub('', text)\n",
        "  # Step 1: replace linebreak characters by white space\n",
        "  text = text.replace('\\n',' ')\n",
        "  \n",
        "  # Step 2: Replace extra white spaces by single white space\n",
        "  spaces = re.compile('\\s+')\n",
        "  text = re.sub(spaces, ' ', text)\n",
        "  #Step 3: Unnecessary characters Removal\n",
        "  text = re.sub(r\"[^a-zA-Z0-9']\", \" \", text) \n",
        "    \n",
        "  if text ==' ': text =''\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3e5L__Rrxcf"
      },
      "source": [
        "Implementation of fuzzy search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zBlvYfSVrxcf"
      },
      "outputs": [],
      "source": [
        "def fuzzy_match(str1=\"\",book_path=None):\n",
        "    best_file=\"\"\n",
        "    best_match = 0\n",
        "    list_ = os.listdir(book_path)\n",
        "    for f in list_:\n",
        "        path_ = book_path +\"/\"+ str(f) \n",
        "        file = open(path_,\"r\")\n",
        "\n",
        "        content = file.read()\n",
        "        ratio = fuzz.token_set_ratio(str1, content)\n",
        "        if best_match < ratio:\n",
        "           best_match = ratio\n",
        "           best_file = f\n",
        "\n",
        "    return best_match, best_file\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfkluK1yrxcg"
      },
      "source": [
        "Implementation of spell_check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SSXaDccLrxcg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def spell_check(text=\"\"):\n",
        "    words = text.split()\n",
        "    final_text=\"\"\n",
        "    for word in words:\n",
        "        misspelled = spell.unknown([word])\n",
        "        if len(misspelled)==1:\n",
        "           correct = spell.correction(word)\n",
        "           if correct == None:\n",
        "              final_text = final_text+ \" \"+ word\n",
        "           else:\n",
        "              final_text = final_text+\" \"+ spell.correction(word)\n",
        "        else:\n",
        "            # Ignore from first empty space\n",
        "            if final_text ==\"\":\n",
        "               final_text = word\n",
        "            else:\n",
        "               final_text = final_text+ \" \"+ word\n",
        "    \n",
        "    return final_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMBKgDprxcg"
      },
      "source": [
        "Implementaion toggle_safe_mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dFfd7cOerxch"
      },
      "outputs": [],
      "source": [
        "def toggle_safe_mode():\n",
        "\n",
        "      msg =\"\\n\\nTwo modes are possible for crawling.\\n\\n\"\\\n",
        "           \"\\033[1mAutomatic:\\033[0m in this mode, the program automatically explores \"\\\n",
        "           \"the web and collects images up to \\nthe maximum number specified by the user, \"\\\n",
        "           \"in this mode, \\033[1munethical\\033[0m images may also be discovered.\\n\\n\"\\\n",
        "           \"\\033[1mSafe:\\033[0m This is a controlled mode, in which 35 images that have \"\\\n",
        "           \"already been collected are processed.\\n\"\n",
        "\n",
        "      print(msg)\n",
        "\n",
        "      print(\"Proceed safely?(\\033[1mY/N\\033[0m)\")\n",
        "\n",
        "      mode = input()\n",
        "      \n",
        "      return mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YenXs09_oSP5"
      },
      "source": [
        "<b>Implementation merge_bounding_boxes</b><br/>\n",
        "The easyocr reader returns the output will be in a list format, each item represents a bounding box, the text detected and confident level, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vcg3YTePvSOe"
      },
      "outputs": [],
      "source": [
        "def merge_bounding_boxes(result:list):\n",
        "  merged=str('')\n",
        "  for item in result:\n",
        "    merged = merged + ' '+ item\n",
        "  return merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k91F43zHrxci"
      },
      "source": [
        "Implementation generate_html_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KEA7Zm9Brxci"
      },
      "outputs": [],
      "source": [
        "def generate_html_table(processed_data=None,directory_name=None):\n",
        "\n",
        "    print(\"\\nGenerateing HTML table started ...\")\n",
        "\n",
        "    if processed_data == None:\n",
        "         print(\"Stoped: No processed data found!\")\n",
        "         return\n",
        "    \n",
        "    count = len(processed_data)\n",
        "    str_rows=\"\"\n",
        "    row_header = \"<tr>\\n\"\\\n",
        "                 \"\\t<td align='center'><h1>Quotes</h1></td>\\n\"\\\n",
        "                 \"\\t<td align='center'><h1>Wallpapers</h1></td>\\n\"\\\n",
        "                 \"\\t<td align='center'><h1>Generated</h1></td>\\n\"\\\n",
        "                 \"</tr>\"\n",
        "\n",
        "    for i in range(count):\n",
        "\n",
        "          source = processed_data[i][\"source_file\"]\n",
        "          # remove parent directory\n",
        "          #source = source.replace(work_directory,'')\n",
        "\n",
        "          wallpaper=processed_data[i][\"wallpaper\"]\n",
        "          #wallpaper = wallpaper.replace(work_directory,\"\")\n",
        "\n",
        "          generated=processed_data[i][\"generated\"]\n",
        "          #generated = generated.replace(work_directory,'')\n",
        "          row_str =\"<tr>\\n\"\\\n",
        "                   \"\\t<td><img src='.\"+source+\"' width='300' height='200'/></td>\\n\"\\\n",
        "                   \"\\t<td><img src='.\"+wallpaper+\"' width='300' height='200'/></td>\\n\"\\\n",
        "                   \"\\t<td><img src='.\"+generated+\"' width='300' height='200'/></td>\\n\"\\\n",
        "                   \"</tr>\"\n",
        "          \n",
        "          str_rows = str_rows + row_str\n",
        "    \n",
        "    print(\"Collected \" +str(count)+\" images ...\")\n",
        "\n",
        "    header_text = \"<h1>Image crawler</h1><br/>\\n\" \\\n",
        "                  \"<b>Abdullah Mohammadi<br/>abdhmohammady@gmail.com</b><br/><br/>\\n\"\\\n",
        "                  \"The result of this table is collected based on crawling the web with the term 'AnneShirleyQuotes'.<br/>\\n\" \\\n",
        "                  \"After collecting about 100 images with the theme of 'Anne Shirley's Quotes', we extracted the<br/>\\n\"\\\n",
        "                  \"text of the images and placed the Persian translation on a new image.<br/><br/>\\n\"\\\n",
        "                  \"In the table below:<br/>\\n\"\\\n",
        "                  \"The 'Quotes' column from the search results of 'AnneShirleyQuotes',<br/>\\n\"\\\n",
        "                  \"The 'Wallpapers' column from the search results of 'sunny' and<br/>\\n\"\\\n",
        "                  \"The 'Generated' column are processed results.<br/><br/>\\n\"\n",
        "\n",
        "    table_str = \"<table align='center' border='1'>\\n\" + row_header + str_rows + \"\\n</table>\"\n",
        "\n",
        "    html_text = header_text + table_str\n",
        "    \n",
        "    #hs = open(directory_name+\"/RESULTS.html\", 'w')\n",
        "    #hs.write(html_text)\n",
        "    #hs.close() \n",
        "    try:\n",
        "      readme = open(directory_name+\"/README.MD\", 'w')\n",
        "      readme.write(html_text)\n",
        "      readme.close() \n",
        "\n",
        "      print(\"HTML table saved in:\", \"/README.MD\")\n",
        "    except Exception as err:\n",
        "      print(f\"Unexpected {err=}, {type(err)=}\")\n",
        "\n",
        "      print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVDgdUXKrxci"
      },
      "source": [
        "Implementation of remove_folder_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_ddSeFIUrxci"
      },
      "outputs": [],
      "source": [
        "def remove_folder_content(folder=None):\n",
        "    for filename in os.listdir(folder):\n",
        "        file_path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ipPuBxrxch"
      },
      "source": [
        "Implementation auto_crawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LAtGrSPdrxch"
      },
      "outputs": [],
      "source": [
        "#Valid toot path must be is 'workspace' or 'workspace' + 'safe_images/'\n",
        "def auto_crawl(max_crawl = 20,quote_path=None,threshold=90, font = None):\n",
        "  \"\"\"\n",
        "  Returns a dictiunary {\n",
        "                        \"undiscovered\":string list of file names,\n",
        "                        \"ignored\":{\"match\": float,\"image_file\":string},\n",
        "                        \"processed\":list of {\"source_file\":string , \"wallpaper\":string , \"generated\":string}\n",
        "                        }\n",
        "  \"\"\"\n",
        "  # The path of the files from which no text is detected is saved here.\n",
        "  undiscovered_files=[]\n",
        "  # The path of the files whose discovered citation matches are less \n",
        "  # than the specified threshold is stored.\n",
        "  ignored_img_list = []\n",
        "  # The path of the files that were processed correctly is saved.\n",
        "  processed_images = []\n",
        "\n",
        "  # Get image names stored in anne shirley qoutes folder\n",
        "  anne_img_names = get_crawled_image_names(quote_path+\"/anne_quotes\")\n",
        "  \n",
        "  # Get image names stored in wallpapers folder\n",
        "  wallpaper_img_names = get_crawled_image_names(quote_path+\"/wallpapers\")\n",
        "  \n",
        "  image_count = min(len(anne_img_names), len(wallpaper_img_names))\n",
        "\n",
        "  if image_count < max_crawl: max_crawl = image_count\n",
        "\n",
        "  for i in range(max_crawl):\n",
        "\n",
        "      anne_img_name      = quote_path + \"/anne_quotes/\" + anne_img_names[i]\n",
        "\n",
        "      wallpaper_img_name = quote_path + \"/wallpapers/\" + wallpaper_img_names[i]\n",
        "\n",
        "      generated_img_name = output_path +\"/\"+ str(i + 1)+\".jpg\"\n",
        "      #\n",
        "      #anne_img = Image.open(anne_img_name)\n",
        "      # detect text from the image\n",
        "      #text = pytesseract.image_to_string(image = anne_img , lang='eng')\n",
        "      print(str(i+1)+\" easyocr reads from: '\"+ anne_img_name.replace(work_directory,\"\")+\"'\")\n",
        "      easyocr_result = easyocr_reader.readtext(anne_img_name, detail = 0)\n",
        "      # The output will be in a list format, each item represents a bounding box, the text detected and confident level, respectively.\n",
        "      text = merge_bounding_boxes(easyocr_result)\n",
        "      #print(\"detected text: \",text)\n",
        "      # Cleaning text \n",
        "      text = clean_text(text = text)\n",
        "      \n",
        "      if text == \"\": \n",
        "        undiscovered_files.append(anne_img_name)\n",
        "        print(\"No text discovered from :\",anne_img_name.replace(work_directory,\"\"))\n",
        "        continue\n",
        "\n",
        "      # Spell Cheching\n",
        "      final_text = spell_check(text=text)\n",
        "      \n",
        "      # Step 3: Call fuzzy_match to check quote validation in anne books\n",
        "      obtained_match,book = fuzzy_match(str1 = final_text,book_path= anne_book_path)\n",
        "\n",
        "      # If matching ratio is less than 90% we ignore this quote and go to next quote\n",
        "      if obtained_match < threshold :\n",
        "        ignored_img_list.append({\"match\": obtained_match,\"image_file\":anne_img_names[i]})\n",
        "\n",
        "        print()\n",
        "        print(\"Ignored due to less than \"+ str(threshold) +\"% text match.\\nfile name: \"+anne_img_name.replace(work_directory,\"\"))\n",
        "        print(\"\\033[1mThe obtained match \" + str(obtained_match)+\"%\\033[0m\")\n",
        "        print()\n",
        "        continue # go to next quote \n",
        "      # Step 3: Translate the text to Persian\n",
        "      translated = translator.translate(text=final_text)\n",
        "\n",
        "      #   End of cleaning text \n",
        "      #_____________________________________________________________________________________________\n",
        "      \n",
        "      # Step 4: Generate output image\n",
        "      \n",
        "      # Step 4.1: Open image from wallpapers\n",
        "      out_img = Image.open(wallpaper_img_name)\n",
        "      #_____________________________________________________________________________________________\n",
        "      # Text placement cumputation \n",
        "    \n",
        "      # Calculate the average length of a single character of our font.\n",
        "      # Note: this takes into account the specific font and font size.\n",
        "      avg_char_width = sum(font.getsize(char)[0] for char in ascii_letters) / len(ascii_letters)\n",
        "      # Translate this average length into a character count\n",
        "      # to fill 95% of our image's total width\n",
        "      max_char_count = int( (out_img.size[0] * .95) / avg_char_width )\n",
        "\n",
        "      # End of Text placement computation\n",
        "      #_____________________________________________________________________________________________\n",
        "\n",
        "      # Create a wrapped text object using scaled character count\n",
        "      scaled_wrapped_text = textwrap.fill(text=translated, width = max_char_count)\n",
        "\n",
        "      #print(\"\\033[1mFrom Image \"+str(i+1)+\": \\033[0m\",text)\n",
        "      #print(\"\\033[1mTranslated: \\033[0m\",scaled_wrapped_text)\n",
        "      #print()#empty line \n",
        "      \n",
        "      # Step 4.2: Call draw Method to add 2D graphics in an image\n",
        "      draw = ImageDraw.Draw(out_img)\n",
        "\n",
        "      # Step 4.3: insert text to the Draw object\n",
        "\n",
        "      #  4.3.1: make shape of text\n",
        "      # correct its shape\n",
        "      #reshaped_text = arabic_reshaper.reshape(translated)    \n",
        "      \n",
        "      #  4.3.2: make direction of text\n",
        "      # correct its direction\n",
        "      #bidi_text = get_display(reshaped_text)                            \n",
        "      \n",
        "      # 4.3.3 Calculate center point of text placement\n",
        "      width  = out_img.width\n",
        "      height = out_img.height\n",
        "      img_center = (width/2,height/2)\n",
        "      \n",
        "      #  4.3.4: Draw text in image\n",
        "      draw.text(xy = img_center,\n",
        "                text = scaled_wrapped_text,\n",
        "                fill=\"Black\",\n",
        "                stroke_fill=\"Gold\",\n",
        "                stroke_width=10,\n",
        "                font=font,\n",
        "                direction=\"rtl\",\n",
        "                language='fa',\n",
        "                anchor ='mm',\n",
        "                Literal='center',\n",
        "                align='right')\n",
        "      \n",
        "      # Step 5: Save output in Generated-Image path\n",
        "      out_img.save(generated_img_name)\n",
        "      \n",
        "      anne_img_name = anne_img_name.replace(work_directory,\"\")\n",
        "      wallpaper_img_name = wallpaper_img_name.replace(work_directory,\"\")\n",
        "      generated_img_name = generated_img_name.replace(work_directory,\"\")\n",
        "\n",
        "      processed_images.append({\"source_file\":anne_img_name,\n",
        "                               \"wallpaper\":wallpaper_img_name ,\n",
        "                               \"generated\":generated_img_name})\n",
        "\n",
        "      print(\"Generated image(text match:\"+str(obtained_match)+\"%) #\",generated_img_name)\n",
        "  \n",
        "  return {\"undiscovered\":undiscovered_files,\"ignored\":ignored_img_list,\"processed\":processed_images}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyvAaGTorxcj"
      },
      "source": [
        "<h1><b>Main body</b></h1>\n",
        "This part is the main body of the code to perform image crawling,\n",
        "Two modes are possible for crawling.<br/>\n",
        "\n",
        "1. Automatic: In this mode, the program automatically explores the web. We can directly crawl and collect citations from the web and then process them. In this method, because we do <b>not monitor</b> the received images, there is a possibility of seeing <b>unethical images</b>. In this case, the number of images is not limited.\n",
        "\n",
        "2. Safe: This is a <b>controlled</b> mode, We have already collected 100 images with 'icrawler' tools, among them we selected 35 images of quotes that have suitable background images, this collection is stored in path '/safe_images', in this case this collection is used for text discovery.\n",
        "\n",
        "In both cases, an attempt is made to extract the text of the image. If no text is discovered, the further processing of that image is prevented, if it is found, it is matched with Anne Shirley's book content. If the matching percentage is higher than <b>specified threshold</b>, the work continues. Otherwise, this image is discarded, the default matching threshold is <b>90%</b>, which can be changed.\n",
        "At the end, a README.MD file is created to show the result of the operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9iCO9C7rxcj",
        "outputId": "2b43e418-b34c-43bf-cbae-fdb52ea7a417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Two modes are possible for crawling.\n",
            "\n",
            "\u001b[1mAutomatic:\u001b[0m in this mode, the program automatically explores the web and collects images up to \n",
            "a maximum number specified by the user, in this mode, \u001b[1munethical\u001b[0m images may also be discovered.\n",
            "\n",
            "\u001b[1mSafe:\u001b[0m This is a controlled mode, in which 35 images that have already been collected are processed.\n",
            "\n",
            "Proceed safely?(\u001b[1mY/N\u001b[0m)\n",
            "y\n",
            "Process started at 8:36:55\n",
            "Max number: 35\n",
            "Threshold: 90\n",
            "Safe mode selected.\n",
            "\n",
            "easyocr reads from: '/safe_images/anne_quotes/32.jpg'\n",
            "Generated image(text match:93%) # /generated_images/1.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/34.jpg'\n",
            "Generated image(text match:100%) # /generated_images/2.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/11.jpg'\n",
            "Generated image(text match:93%) # /generated_images/3.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/12.jpg'\n",
            "Generated image(text match:94%) # /generated_images/4.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/26.jpg'\n",
            "Generated image(text match:96%) # /generated_images/5.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/35.jpg'\n",
            "Generated image(text match:98%) # /generated_images/6.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/19.jpg'\n",
            "Generated image(text match:98%) # /generated_images/7.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/25.jpg'\n",
            "Generated image(text match:96%) # /generated_images/8.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/15.jpg'\n",
            "Generated image(text match:91%) # /generated_images/9.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/31.jpg'\n",
            "Generated image(text match:90%) # /generated_images/10.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/17.jpg'\n",
            "Generated image(text match:95%) # /generated_images/11.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/27.jpg'\n",
            "Generated image(text match:91%) # /generated_images/12.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/29.jpg'\n",
            "\n",
            "Ignored due to less than 90% text match.\n",
            "file name: /safe_images/anne_quotes/29.jpg\n",
            "\u001b[1mThe obtained match 86%\u001b[0m\n",
            "\n",
            "easyocr reads from: '/safe_images/anne_quotes/10.jpg'\n",
            "Generated image(text match:93%) # /generated_images/14.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/20.jpg'\n",
            "Generated image(text match:100%) # /generated_images/15.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/28.jpg'\n",
            "Generated image(text match:100%) # /generated_images/16.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/23.jpg'\n",
            "Generated image(text match:92%) # /generated_images/17.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/16.jpg'\n",
            "Generated image(text match:92%) # /generated_images/18.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/24.jpg'\n",
            "Generated image(text match:92%) # /generated_images/19.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/30.jpg'\n",
            "Generated image(text match:98%) # /generated_images/20.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/22.jpg'\n",
            "Generated image(text match:96%) # /generated_images/21.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/33.jpg'\n",
            "Generated image(text match:97%) # /generated_images/22.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/14.jpg'\n",
            "Generated image(text match:90%) # /generated_images/23.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/13.jpg'\n",
            "Generated image(text match:93%) # /generated_images/24.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/1.jpg'\n",
            "Generated image(text match:96%) # /generated_images/25.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/2.jpg'\n",
            "\n",
            "Ignored due to less than 90% text match.\n",
            "file name: /safe_images/anne_quotes/2.jpg\n",
            "\u001b[1mThe obtained match 82%\u001b[0m\n",
            "\n",
            "easyocr reads from: '/safe_images/anne_quotes/3.jpg'\n",
            "Generated image(text match:95%) # /generated_images/27.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/4.jpg'\n",
            "Generated image(text match:92%) # /generated_images/28.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/5.jpg'\n",
            "\n",
            "Ignored due to less than 90% text match.\n",
            "file name: /safe_images/anne_quotes/5.jpg\n",
            "\u001b[1mThe obtained match 82%\u001b[0m\n",
            "\n",
            "easyocr reads from: '/safe_images/anne_quotes/6.jpg'\n",
            "Generated image(text match:97%) # /generated_images/30.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/7.jpg'\n",
            "Generated image(text match:99%) # /generated_images/31.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/8.jpg'\n",
            "Generated image(text match:95%) # /generated_images/32.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/9.jpg'\n",
            "Generated image(text match:91%) # /generated_images/33.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/18.jpg'\n",
            "Generated image(text match:92%) # /generated_images/34.jpg\n",
            "easyocr reads from: '/safe_images/anne_quotes/21.jpg'\n",
            "Generated image(text match:96%) # /generated_images/35.jpg\n",
            "\n",
            "-------------------------------------------------------------------------------------\n",
            "\u001b[1mText matching threshold\u001b[0m: 90\n",
            "\u001b[1mProcessed images\u001b[0m: 32\n",
            "\u001b[1mIgnored images\u001b[0m: 3\n",
            "29.jpg, obtained ratio :  86%\n",
            "2.jpg, obtained ratio :  82%\n",
            "5.jpg, obtained ratio :  82%\n",
            "\u001b[1mUndiscovered images\u001b[0m :  0\n",
            "Generateing HTML table started ...\n",
            "Collected 32 images ...\n",
            "HTML table saved in: /README.MD\n",
            "All tasks done at 8:58:45\n"
          ]
        }
      ],
      "source": [
        "\n",
        "max_number = 35\n",
        "threshold = 90\n",
        "#Toggles Safe mode and Automatic Made\n",
        "mode =  toggle_safe_mode()\n",
        "# Automatic: in this mode, the program automatically explores the web and collects \n",
        "# images up to a maximum number specified in the 'Initalization' cell.\n",
        "# in this mode,unethical images may also be discovered.\n",
        "# Safe: This is a controlled mode, in which 35 images that have already been collected are processed.\n",
        "\n",
        "n = datetime.datetime.now()\n",
        "\n",
        "print (\"Process started at %s:%s:%s\" % (n.hour, n.minute, n.second))\n",
        "\n",
        "print(\"Max number:\", max_number)\n",
        "\n",
        "print(\"Threshold:\",threshold)\n",
        "\n",
        "if mode not in [\"Y\",\"y\",\"N\",\"n\"]:\n",
        "    sys.exit('Terminate by wrong keywords!')\n",
        "\n",
        "workspace = os.getcwd()\n",
        "# Calling os.getcwd() has the following results:\n",
        "# In google colab it returns '/content', This is the root directory.\n",
        "# In GitHub codespace returns '/workspaces/codespaces-jupyter' \n",
        "# but this is the work directory.\n",
        "\n",
        "# In GitHub codespace, our work directory is '/workspaces/codespaces-jupyter'\n",
        "work_directory = workspace +\"\"\n",
        "\n",
        "# ENABLE THIS LINE IN 'Google Colab' WORKSPACE\n",
        "# DISABLE THIS LINE IN GitHub' CODESPACE\n",
        "# In google colab, our work directory is '/content/drive/MyDrive/Colab Notebooks/Text-Mining'\n",
        "#work_directory = workspace + \"/drive/MyDrive/Colab Notebooks/Text-Mining\"\n",
        "\n",
        "# When workspace and work directory was specified ...\n",
        "font_path = work_directory+\"/fonts\"\n",
        "anne_book_path = work_directory+\"/anne_books\"\n",
        "# Directory to generate wallpapers from crawled images\n",
        "output_path = work_directory+\"/generated_images\"\n",
        "\n",
        "if not os.path.exists(output_path): os.makedirs(output_path)\n",
        "else: remove_folder_content(output_path)\n",
        "\n",
        "# Safe crawling is requested\n",
        "if mode in [\"Y\",\"y\"]:\n",
        "   print(\"Safe mode selected.\\n\")\n",
        "   # Defualt 'anne_quote' and 'wallpapers' folders located here.\n",
        "   quote_path = work_directory + \"/safe_images\"\n",
        "# Automatic crawling is requested, In automatic mode:\n",
        "# In the first step, the images of Anne Shirley are searched\n",
        "# In the second name step, the folder of the found images is renamed to the folder of 'anne_quotes.\n",
        "# In the third step, wallpaper images are searched\n",
        "# In the fourth step, the name of the folder of the found images is changed to that of 'wallpapers'.\n",
        "elif mode in [\"N\",\"n\"]:\n",
        "   print(\"Automatic mode selected.\\n\")\n",
        "   quote_path = workspace\n",
        "   # When 'anne qoutes' crawled,by defualt,crawler creates 'images' folder in root directory\n",
        "   # to manage root diregtory, we store it in 'workspace' varible.\n",
        "   # Clear content of 'images' folder\n",
        "   if os.path.exists(quote_path+\"/images\"):\n",
        "      #remove_folder_content(workspace+\"/images\")\n",
        "      # deletes the directory and all its contents.\n",
        "      shutil.rmtree(quote_path +\"/images\")\n",
        "   \n",
        "   # Step 1: Crawling starts\n",
        "   bing_crawler = bing.BingImageCrawler()\n",
        "   bing_crawler.crawl(keyword = anne_keyword, offset = 0, max_num = max_number, min_size = None, max_size = None)\n",
        "   #by default the crawler creates a folder as 'images' in root directory\n",
        "   # and stores crawled images in, we rename this folder to 'anne_quotes'\n",
        "   # becuase we want crawl wallpapers again.\n",
        "    \n",
        "   # Step 2: Rename crawled 'images' folder to 'anne_quotes'\n",
        "   if os.path.exists(quote_path +\"/anne_quotes\"):\n",
        "      #remove_folder_content(workspace +\"/anne_quotes\")\n",
        "      shutil.rmtree(quote_path +\"/anne_quotes\")\n",
        "\n",
        "   os.rename(quote_path+\"/images\",quote_path + \"/anne_quotes\")\n",
        "   \n",
        "   # Step 3: Wallpapers Crawling starts\n",
        "   #google_crawler = google.GoogleImageCrawler()\n",
        "   bing_crawler.crawl(keyword = wallpaper_keyword, offset = 0, max_num = max_number, min_size = None, max_size = None)\n",
        "   #by default the crawler creates a folder as 'images' in root directory\n",
        "   # and stores crawled images in, we rename this folder to 'wallpapers'\n",
        "   \n",
        "   # Step 4: Rename crawled 'images' folder to 'wallpapers'\n",
        "   if os.path.exists(quote_path +\"/wallpapers\"):\n",
        "      shutil.rmtree(quote_path +\"/wallpapers\")\n",
        "   \n",
        "   if os.path.exists(quote_path+\"/images\"):\n",
        "      os.rename(quote_path+\"/images\",quote_path +\"/wallpapers\")\n",
        "\n",
        "font_ = ImageFont.truetype(font = font_path +'/Vazirmatn-Regular.ttf',size= 70)\n",
        "\n",
        "result = auto_crawl(max_crawl= max_number , quote_path = quote_path , threshold=threshold, font = font_)\n",
        "\n",
        "generate_html_table(processed_data = result[\"processed\"],directory_name = work_directory)\n",
        "\n",
        "processed = len(result[\"processed\"])\n",
        "\n",
        "ignored = len(result[\"ignored\"])\n",
        "\n",
        "undiscovered = len(result[\"undiscovered\"])\n",
        "\n",
        "print()\n",
        "print(\"-------------------------------------------------------------------------------------\")\n",
        "print(\"\\033[1mText matching threshold\\033[0m:\",threshold)\n",
        "print(\"\\033[1mProcessed images\\033[0m:\",processed)\n",
        "print(\"\\033[1mIgnored images\\033[0m:\",ignored)\n",
        "\n",
        "for item in result[\"ignored\"]:\n",
        "    print(item[\"image_file\"]+\", obtained ratio : \",str(item[\"match\"])+\"%\")\n",
        "\n",
        "print(\"\\033[1mUndiscovered images\\033[0m : \",undiscovered)\n",
        "\n",
        "for item in result[\"undiscovered\"]:\n",
        "       print(item)\n",
        "n = datetime.datetime.now()\n",
        "print (\"All tasks done at %s:%s:%s\" % (n.hour, n.minute, n.second))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
