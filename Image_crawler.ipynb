{"cells":[{"cell_type":"markdown","metadata":{"id":"e7pf16OWrxcR"},"source":["<h1>Image crawler</h1>\n","\n","<div>In this notebook, we are going to find pictures of Anne Shirley's quotes by crawling the web, extract the text of the pictures and translate them into Persian, then paste the translated text on the new pictures. We do this process by searching the web using image crawlers. Many tools can be found for this task provided by various developers, whatever we use in this notebook also mention its source. We do this in the following steps.</div>\n","\n","<ul style='list-style-type:square'>\n","    <li>Crawling Anne Shirley's quotes</li>\n","    <li>Extracting text from images</li>\n","    <li>Spell checking</li>\n","    <li>Cleaning the texts</li>\n","    <li>Checking the validation of the text</li>\n","    <li>Translating</li>\n","    <li>Crawling new another images</li>\n","    <li>Inserting the texts in the context of new images</li>\n","    <li>Display the results</li>\n","</ul>\n","\n","<b>Introduction</b><br/>\n","<style>  \n","  div {padding-right:25px; padding-left: 25px;}  \n","</style>\n","<div>A Web crawler, sometimes called a <b>spider</b> or <b>spiderbot</b> and often shortened to <b>crawler</b>, is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing (web spidering).<a href=\"https://en.wikipedia.org/wiki/Web_crawler#cite_note-1\">[1]</a></div><br/>\n","<div>The image crawler application is used to collect a multitude of images from websites. The images can be viewed as thumbnails or saved to a given folder for enhanced processing.<a href=\"https://sourceforge.net/projects/imagecrawler/\">[2]</a><br/>\n","</div>\n","\n","<b>Contents</b><br/>\n","<ul>\n","  <li>Installations</li>\n","      <ul>\n","          <li>icrawler</li>\n","          <li>pytesseract</li>\n","          <li>pyspellchecker</li>\n","          <li>fuzzywuzzy</li>\n","          <li>deep-translator</li>\n","          <li> if necessary, install arabic_reshaper and python-bidi</li>\n","      </ul>\n","  <li>Imporl libraries</li>\n","  <li>Implementation helper functions</li>\n","  <li>Crawl</li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"cEHuSwuzrxcX"},"source":["<h1><b>Installations</b></h1>"]},{"cell_type":"markdown","metadata":{"id":"7iyqZC04rxcX"},"source":["<b>Install icrawler</b><br/>\n","In this project, we use the icrawler library, if you haven't installed it yet, you should install it(Python 3.5+ recommended).<br/>\n","With this package, you can write a multiple thread crawler easily by focusing on the contents you want to crawl, keeping away from troublesome problems like exception handling, thread scheduling and communication.<a href='https://pypi.org/project/icrawler/'>[3]</a>\n","\n","It also provides built-in crawlers for popular image sites like Flickr and search engines such as Google, Bing and Baidu.<a href='https://pypi.org/project/icrawler/'>[4]</a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5877,"status":"ok","timestamp":1679731020454,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"DC_G76d4rxcY","outputId":"c52bbe37-1e14-48bd-dd78-46111c23ff9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting icrawler\n","  Downloading icrawler-0.6.6-py2.py3-none-any.whl (35 kB)\n","Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.9/dist-packages (from icrawler) (2.27.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from icrawler) (1.16.0)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from icrawler) (4.11.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from icrawler) (8.4.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from icrawler) (4.9.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.4.1->icrawler) (2.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->icrawler) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->icrawler) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->icrawler) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.1->icrawler) (3.4)\n","Installing collected packages: icrawler\n","Successfully installed icrawler-0.6.6\n"]}],"source":["# if you haven't installed it yet, you should install it with the following command.\n","# Run this line just first time\n","%pip install icrawler"]},{"cell_type":"markdown","metadata":{"id":"WU7xAWHdrxcZ"},"source":["<h1>Install pytesseract</h1><br/>\n","For enabling our python program to have character recognition capabilities, we would be making use of pytesseract OCR library. The library could be installed onto our python environment by executing the command \"%pip install pytesseract\" in the command interpreter of the OS.<a href=\"https://www.geeksforgeeks.org/how-to-extract-text-from-images-with-python/\">[5]</a>\n","\n","The library (if used on Windows OS) requires the tesseract.exe binary to be also present for proper installation of the library. During the installation of the aforementioned executable, we would be prompted to specify a path for it. This path needs to be remembered as it would be utilized later on in the code.<a href=\"https://www.geeksforgeeks.org/how-to-extract-text-from-images-with-python/\">[6]</a>\n","\n","Normally Tesseract has been developed on Debian GNU Linux, but there is also the need for a Windows version. That's why has been built a Tesseract installer for Windows.<a href=\"https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\">[7]</a>\n","\n","Therefore, to use in Windows, download its installer file from <a href=\"https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\">here</a>.\n","\n","<b>WARNING:</b> After installation, To use this ocr correctly, the pytesseract.tesseract_cmd variable must be assigned the installation path (this path is used by the library to find the executable and use it for extraction)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12904,"status":"ok","timestamp":1679731078009,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"ISPGRJtMrxca","outputId":"e55b0559-f415-4fff-9bc9-1e44f31e5603"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  tesseract-ocr-eng tesseract-ocr-osd\n","The following NEW packages will be installed:\n","  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n","0 upgraded, 3 newly installed, 0 to remove and 23 not upgraded.\n","Need to get 4,850 kB of archives.\n","After this operation, 16.3 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1 [1,598 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1 [2,990 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr amd64 4.1.1-2build2 [262 kB]\n","Fetched 4,850 kB in 1s (4,342 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package tesseract-ocr-eng.\n","(Reading database ... 128285 files and directories currently installed.)\n","Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1_all.deb ...\n","Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n","Selecting previously unselected package tesseract-ocr-osd.\n","Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1_all.deb ...\n","Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n","Selecting previously unselected package tesseract-ocr.\n","Preparing to unpack .../tesseract-ocr_4.1.1-2build2_amd64.deb ...\n","Unpacking tesseract-ocr (4.1.1-2build2) ...\n","Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n","Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n","Setting up tesseract-ocr (4.1.1-2build2) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from pytesseract) (23.0)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from pytesseract) (8.4.0)\n","Installing collected packages: pytesseract\n","Successfully installed pytesseract-0.3.10\n"]}],"source":["!sudo apt install tesseract-ocr\n","%pip install pytesseract"]},{"cell_type":"markdown","metadata":{"id":"eEKYj1cjrxca"},"source":["<b>pyspellchecker</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7097,"status":"ok","timestamp":1679730952597,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"o7pFAA5vrxcb","outputId":"ccb25683-9241-4b6c-c3e5-a91549d5e5fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspellchecker\n","  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.7.1\n"]}],"source":["%pip install pyspellchecker"]},{"cell_type":"markdown","metadata":{"id":"0AiTt13mrxcb"},"source":["<b>fuzzywuzzy</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15048,"status":"ok","timestamp":1679730993207,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"BJmpUP-1rxcb","outputId":"70baebc1-51f2-4bef-8506-540f5145bf03"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fuzzywuzzy\n","  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n","Installing collected packages: fuzzywuzzy\n","Successfully installed fuzzywuzzy-0.18.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting python-Levenshtein\n","  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n","Collecting Levenshtein==0.20.9\n","  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n","  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n","Successfully installed Levenshtein-0.20.9 python-Levenshtein-0.20.9 rapidfuzz-2.13.7\n"]}],"source":["%pip install fuzzywuzzy\n","%pip install python-Levenshtein"]},{"cell_type":"markdown","metadata":{"id":"85ZZHw0nrxcc"},"source":["<b>deep-translator</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4839,"status":"ok","timestamp":1679731044690,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"q4klXNYRrxcc","outputId":"a6c43fd7-1436-4f45-9217-719021c63921"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting deep-translator\n","  Downloading deep_translator-1.10.1-py3-none-any.whl (35 kB)\n","Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from deep-translator) (2.27.1)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from deep-translator) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.12.7)\n","Installing collected packages: deep-translator\n","Successfully installed deep-translator-1.10.1\n"]}],"source":["%pip install deep-translator"]},{"cell_type":"markdown","metadata":{"id":"U-FjSzZirxcc"},"source":["<b>arabic_reshaper and python-didi</b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nRut2RByrxcd"},"outputs":[],"source":["#!pip install arabic_reshaper\n","#!pip install python-bidi"]},{"cell_type":"markdown","metadata":{"id":"qwHbmglLrxcd"},"source":["<h1><b>Import libraries</b></h1>"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":642,"status":"ok","timestamp":1679740474479,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"F6nxx8ovrxcd"},"outputs":[],"source":["import datetime\n","import os,shutil\n","import re\n","from spellchecker import SpellChecker\n","#Options for spell cheking\n","# %pip uninstall PyEnchant\n","# %pip install textblo\n","# %pip install jamspell\n","# %pip install autocorrect\n","\n","from fuzzywuzzy import fuzz\n","from icrawler.builtin import bing\n","from icrawler.builtin import google \n","#Other options\n","# from icrawler.builtin import baidu\n","# from icrawler.builtin import flickr\n","\n","from typing import Literal\n","#import arabic_reshaper\n","#from bidi.algorithm import get_display\n","from string import ascii_letters\n","import textwrap\n","from PIL import ImageFont\n","from PIL import Image\n","from PIL import ImageDraw\n","from deep_translator import GoogleTranslator\n","import pytesseract\n"]},{"cell_type":"markdown","metadata":{"id":"wBR1TqHirxcd"},"source":["<h1>Initailize</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-11pz5MGrxce"},"outputs":[],"source":["workspace = \"\"\n","root_directory =\"\"\n","quote_path = \"\"\n","output_path=\"\"\n","wallpaper_keyword = \"sunny\"\n","anne_keyword=\"AnneShirleyQuotes\"\n","\n","max_number = 35\n","threshold = 75\n","\n","pytesseract.pytesseract.tesseract_cmd =(r'/usr/bin/tesseract')\n","#Whe local: 'C:/Program Files/Tesseract-OCR/tesseract.exe' \n","\n","translator = GoogleTranslator(source='auto', target='fa')\n","\n","spell = SpellChecker()"]},{"cell_type":"markdown","metadata":{"id":"3-Dol8J7rxce"},"source":["<h1><b>Implementation helper functions</b></h1>"]},{"cell_type":"markdown","metadata":{"id":"WDQmL5Berxce"},"source":["Implementation of get_crawled_image_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHqtKmkXrxce"},"outputs":[],"source":["# returns crawled images directory and list of images in\n","def get_crawled_image_names(parent_path=None):\n","    # get list of crawled images\n","    # Note: crawled imaged by icrawler is stored in a folder named as 'Images'\n","    # in root directory\n","    list_ = os.listdir(parent_path)\n","    # filter files in the 'Images' folder by valid extensions\n","    ext = ['jpg','jpeg','png','bmp']\n","\n","    # Filter file names based on given extensions in list 2\n","    filtered = [x for x in list_ if any(y in x for y in ext)]\n","    \n","    # Returns crawled image names as list\n","    return filtered\n"]},{"cell_type":"markdown","metadata":{"id":"JVTwesA2rxcf"},"source":["Implementation of text cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TrNebQdrxcf"},"outputs":[],"source":["extra_spaces_pattern = re.compile('\\s+')\n","\n","def clean_text(text=\"\"):\n","    # Step 1: replace linebreak characters by white space\n","    text = text.replace('\\n',' ')\n","    # Step 2: Replace extra white spaces by single white space\n","    text = re.sub(extra_spaces_pattern, ' ', text)\n","    #Step 3: Unnecessary characters Removal\n","    text = re.sub(r\"[^a-zA-Z0-9',]\", \" \", text) \n","    \n","    if text ==\" \": text =\"\"\n","\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"L3e5L__Rrxcf"},"source":["Implementation of fuzzy search"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":605,"status":"ok","timestamp":1679740773114,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"zBlvYfSVrxcf"},"outputs":[],"source":["def fuzzy_match(str1=\"\",book_path=None):\n","    best_file=\"\"\n","    best_match = 0\n","    list_ = os.listdir(book_path)\n","    for f in list_:\n","        path_ = book_path +\"/\"+ str(f) \n","        file = open(path_,\"r\")\n","\n","        content = file.read()\n","        ratio = fuzz.token_set_ratio(str1, content)\n","        if best_match < ratio:\n","           best_match = ratio\n","           best_file = f\n","\n","    return best_match, best_file\n","    "]},{"cell_type":"markdown","metadata":{"id":"xfkluK1yrxcg"},"source":["Implementation of spell_check"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":396,"status":"ok","timestamp":1679740768051,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"SSXaDccLrxcg"},"outputs":[],"source":["\n","def spell_check(text=\"\"):\n","    words = text.split()\n","    final_text=\"\"\n","    for word in words:\n","        misspelled = spell.unknown([word])\n","        if len(misspelled)==1:\n","           correct = spell.correction(word)\n","           if correct == None:\n","              final_text = final_text+ \" \"+ word\n","           else:\n","              final_text = final_text+\" \"+ spell.correction(word)\n","        else:\n","            # Ignore from first empty space\n","            if final_text ==\"\":\n","               final_text = word\n","            else:\n","               final_text = final_text+ \" \"+ word\n","    \n","    return final_text\n"]},{"cell_type":"markdown","metadata":{"id":"jEMBKgDprxcg"},"source":["Implementaion toggle_safe_mode"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":624,"status":"ok","timestamp":1679740759459,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"dFfd7cOerxch"},"outputs":[],"source":["def toggle_safe_mode():\n","\n","      msg =\"Two modes are possible for crawling.\\n\\n\"\\\n","           \"\\033[1mAutomatic:\\033[0m in this mode, the program automatically explores \"\\\n","           \"the web and collects images up to \\na maximum number specified by the user, \"\\\n","           \"in this mode, \\033[1munethical\\033[0m images may also be discovered.\\n\\n\"\\\n","           \"\\033[1mSafe:\\033[0m This is a controlled mode, in which 35 images that have \"\\\n","           \"already been collected are processed.\\n\"\n","\n","      print(msg)\n","\n","      print(\"Proceed safely?(\\033[1mY/N\\033[0m)\")\n","\n","      mode = input()\n","      \n","      return mode\n"]},{"cell_type":"markdown","metadata":{"id":"d5ipPuBxrxch"},"source":["Implementation auto_crawl"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":646,"status":"ok","timestamp":1679740748988,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"LAtGrSPdrxch"},"outputs":[],"source":["#Valid toot path must be is 'workspace' or 'workspace' + 'safe_images/'\n","def auto_crawl(max_crawl = 20,quote_path=None,threshold=80, font = None):\n","  \"\"\"\n","  Returns a dictiunary {\n","                        \"undiscovered\":string list of file names,\n","                        \"ignored\":{\"match\": float,\"image_file\":string},\n","                        \"processed\":list of {\"source_file\":string , \"wallpaper\":string , \"generated\":string}\n","                        }\n","  \"\"\"\n","  # The path of the files from which no text is detected is saved here.\n","  undiscovered_files=[]\n","  # The path of the files whose discovered citation matches are less \n","  # than the specified threshold is stored.\n","  ignored_img_list = []\n","  # The path of the files that were processed correctly is saved.\n","  processed_images = []\n","\n","  # Get image names stored in anne shirley qoutes folder\n","  anne_img_names = get_crawled_image_names(quote_path+\"/anne_quotes\")\n","  \n","  # Get image names stored in wallpapers folder\n","  wallpaper_img_names = get_crawled_image_names(quote_path+\"/wallpapers\")\n","  \n","  image_count = min(len(anne_img_names), len(wallpaper_img_names))\n","\n","  if image_count < max_crawl: max_crawl = image_count\n","\n","  for i in range(max_crawl):\n","\n","      anne_img_name      = quote_path + \"/anne_quotes/\" + anne_img_names[i]\n","\n","      wallpaper_img_name = quote_path + \"/wallpapers/\" + wallpaper_img_names[i]\n","\n","      generated_img_name = output_path +\"/\"+ str(i + 1)+\".jpg\"\n","\n","      anne_img = Image.open(anne_img_name)\n","      # detect text from the image\n","      text = pytesseract.image_to_string(image = anne_img , lang='eng')\n","\n","      # Cleaning text \n","      text = clean_text(text = text)\n","      \n","      if text == \"\": \n","        undiscovered_files.append(anne_img_name)\n","        continue\n","\n","      # Spell Cheching\n","      final_text = spell_check(text=text)\n","      \n","      # Step 3: Call fuzzy_match to check quote validation in anne books\n","      obtained_match,book = fuzzy_match(str1 = final_text,book_path= anne_book_path)\n","\n","      # If matching ratio is less than 90% we ignore this quote and go to next quote\n","      if obtained_match < threshold :\n","\n","        ignored_img_list.append({\"match\": obtained_match,\"image_file\":anne_img_name})\n","\n","        print()\n","        print(\"Ignored due to less than \"+ str(threshold) +\"% match.\\n\"+anne_img_name)\n","        print(\"\\033[1mThe obtained match \" + str(obtained_match)+\"%\\033[0m\")\n","        print()\n","        continue # go to next quote \n","      # Step 3: Translate the text to Persian\n","      translated = translator.translate(text=final_text)\n","\n","      #   End of cleaning text \n","      #_____________________________________________________________________________________________\n","      \n","      # Step 4: Generate output image\n","      \n","      # Step 4.1: Open image from wallpapers\n","      out_img = Image.open(wallpaper_img_name)\n","      #_____________________________________________________________________________________________\n","      # Text placement cumputation \n","    \n","      # Calculate the average length of a single character of our font.\n","      # Note: this takes into account the specific font and font size.\n","      avg_char_width = sum(font.getsize(char)[0] for char in ascii_letters) / len(ascii_letters)\n","      # Translate this average length into a character count\n","      # to fill 95% of our image's total width\n","      max_char_count = int( (out_img.size[0] * .95) / avg_char_width )\n","\n","      # End of Text placement computation\n","      #_____________________________________________________________________________________________\n","\n","      # Create a wrapped text object using scaled character count\n","      scaled_wrapped_text = textwrap.fill(text=translated, width = max_char_count)\n","\n","      #print(\"\\033[1mFrom Image \"+str(i+1)+\": \\033[0m\",text)\n","      #print(\"\\033[1mTranslated: \\033[0m\",scaled_wrapped_text)\n","      #print()#empty line \n","      \n","      # Step 4.2: Call draw Method to add 2D graphics in an image\n","      draw = ImageDraw.Draw(out_img)\n","\n","      # Step 4.3: insert text to the Draw object\n","\n","      #  4.3.1: make shape of text\n","      # correct its shape\n","      #reshaped_text = arabic_reshaper.reshape(translated)    \n","      \n","      #  4.3.2: make direction of text\n","      # correct its direction\n","      #bidi_text = get_display(reshaped_text)                            \n","      \n","      # 4.3.3 Calculate center point of text placement\n","      width  = out_img.width\n","      height = out_img.height\n","      img_center = (width/2,height/2)\n","      \n","      #  4.3.4: Draw text in image\n","      draw.text(xy = img_center,\n","                text = scaled_wrapped_text,\n","                fill=\"Black\",\n","                stroke_fill=\"Gold\",\n","                stroke_width=10,\n","                font=font,\n","                direction=\"rtl\",\n","                language='fa',\n","                anchor ='mm',\n","                Literal='center',\n","                align='right')\n","      \n","      # Step 5: Save output in Generated-Image path\n","      \n","      processed_images.append({\"source_file\":anne_img_name,\"wallpaper\":wallpaper_img_name ,\"generated\":generated_img_name})\n","\n","      out_img.save(generated_img_name)\n","\n","      print(\"Generated image #\",generated_img_name)\n","  \n","  return {\"undiscovered\":undiscovered_files,\"ignored\":ignored_img_list,\"processed\":processed_images}\n"]},{"cell_type":"markdown","metadata":{"id":"k91F43zHrxci"},"source":["Implementation generate_html_table"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":775,"status":"ok","timestamp":1679740778932,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"KEA7Zm9Brxci"},"outputs":[],"source":["def generate_html_table(processed_data=None,directory_name=None):\n","\n","    print(\"Generateing HTML table started ...\")\n","\n","    if processed_data == None:\n","         print(\"Stoped: No processed data found!\")\n","         return\n","    \n","    count = len(processed_data)\n","    str_rows=\"\"\n","    row_header = \"<tr>\\n\"\\\n","                    \"<td><h1>Quotes</h1></td>\\n\"\\\n","                    \"<td><h1>Wallpapers</h1></td><td>\\n\"\\\n","                    \"<h1>Generated</h1></td>\\n\"\\\n","                  \"</tr>\"\n","\n","    for i in range(count):\n","\n","          source = processed_data[i][\"source_file\"]\n","\n","          wallpaper=processed_data[i][\"wallpaper\"]\n","          generated=processed_data[i][\"generated\"]\n","\n","          row_str =\"<tr>\\n\"\\\n","                       \"<td><img src='\"+source+\"' width='300'/></td>\\n\"\\\n","                       \"<td><img src='\"+wallpaper+\"' width='300'/></td>\\n\"\\\n","                       \"<td><img src='\"+generated+\"' width='300'/></td>\\n\"\\\n","                    \"</tr>\"\n","          \n","          str_rows = str_rows + row_str\n","    \n","    print(\"Collected \" +str(count)+\" images ...\")\n","\n","    header_text = \"<h1>Image crawler</h1><br/>\\n\" \\\n","                  \"The result of this table is collected based on crawling the web with the term 'AnneShirleyQuoes'.<br/>\\n\" \\\n","                  \"After collecting about 100 images with the theme of 'Quotes of Anne Shirley', we extracted the<br/>\\n\"\\\n","                  \"text of the images and placed the Persian translation on a new image.<br/>\\n\"\\\n","                  \"In the table below:<br/>\\n\"\\\n","                  \"The 'Quotes' column from the search results of 'AnneShirleyQuoes',<br/>\\n\"\\\n","                  \"The 'Wallpapers' column from the search results of 'sunny' and<br/>\\n\"\\\n","                  \"The 'Generated' column are processed results.<br/>\\n\"\\\n","                  \"<b>Abdullah Mohammadi<br/>abdhmohammady@gmail.com</b><br/><br/>\\n\"\n","\n","    table_str = \"<table align='center' border='1'>\\n\" + row_header + str_rows + \"\\n</table>\"\n","\n","    html_text = header_text + table_str\n","    \n","    #hs = open(directory_name+\"/RESULTS.html\", 'w')\n","    #hs.write(html_text)\n","    #hs.close() \n","    try:\n","      readme = open(directory_name+\"/README.MD\", 'w')\n","      readme.write(html_text)\n","      readme.close() \n","\n","      print(\"HTML table saved in:\", directory_name+\"/README.MD\")\n","    except Exception as err:\n","      print(f\"Unexpected {err=}, {type(err)=}\")\n","\n","      print()\n"]},{"cell_type":"markdown","metadata":{"id":"QVDgdUXKrxci"},"source":["Implementation of remove_folder_content"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":593,"status":"ok","timestamp":1679740783490,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"_ddSeFIUrxci"},"outputs":[],"source":["def remove_folder_content(folder=None):\n","    for filename in os.listdir(folder):\n","        file_path = os.path.join(folder, filename)\n","        try:\n","            if os.path.isfile(file_path) or os.path.islink(file_path):\n","                os.unlink(file_path)\n","            elif os.path.isdir(file_path):\n","                shutil.rmtree(file_path)\n","        except Exception as e:\n","            print('Failed to delete %s. Reason: %s' % (file_path, e))"]},{"cell_type":"markdown","metadata":{"id":"xyvAaGTorxcj"},"source":["<h1><b>Main</b></h1>"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110073,"status":"ok","timestamp":1679740909103,"user":{"displayName":"Abdh Mohammadi","userId":"01027356643191143681"},"user_tz":-210},"id":"h9iCO9C7rxcj","outputId":"be47a39d-a8c9-4b60-f2be-df71923f0a45"},"outputs":[{"name":"stdout","output_type":"stream","text":["Two modes are possible for crawling.\n","\n","\u001b[1mAutomatic:\u001b[0m in this mode, the program automatically explores the web and collects images up to \n","a maximum number specified by the user, in this mode, \u001b[1munethical\u001b[0m images may also be discovered.\n","\n","\u001b[1mSafe:\u001b[0m This is a controlled mode, in which 35 images that have already been collected are processed.\n","\n","Proceed safely?(\u001b[1mY/N\u001b[0m)\n","Y\n","Process started at 11:40:7\n","Mode:  Y\n","Max number: 35\n","Threshold: 75\n","Safe mode selected.\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/1.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/2.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/3.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/4.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/5.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/6.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/7.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/9.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/10.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/11.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/12.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/13.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/14.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/15.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/16.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/17.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/19.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/20.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/21.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/22.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/23.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/24.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/25.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/26.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/27.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/28.jpg\n","\n","Ignored due to less than 75% match.\n","/content/drive/MyDrive/Colab Notebooks/Text-Mining/safe_images/anne_quotes/5.jpg\n","\u001b[1mThe obtained match 0%\u001b[0m\n","\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/30.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/31.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/32.jpg\n","\n","Ignored due to less than 75% match.\n","/content/drive/MyDrive/Colab Notebooks/Text-Mining/safe_images/anne_quotes/9.jpg\n","\u001b[1mThe obtained match 64%\u001b[0m\n","\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/34.jpg\n","Generated image # /content/drive/MyDrive/Colab Notebooks/Text-Mining/generated_images/35.jpg\n","\n","-------------------------------------------------------------------------------------\n","\u001b[1mText matching threshold\u001b[0m: 75\n","\u001b[1mProcessed images\u001b[0m: 31\n","\u001b[1mIgnored images\u001b[0m: 2\n","/content/drive/MyDrive/Colab Notebooks/Text-Mining/safe_images/anne_quotes/5.jpg, obtained ratio :  0%\n","/content/drive/MyDrive/Colab Notebooks/Text-Mining/safe_images/anne_quotes/9.jpg, obtained ratio :  64%\n","\u001b[1mUndiscovered images\u001b[0m :  2\n","/content/drive/MyDrive/Colab Notebooks/Text-Mining/safe_images/anne_quotes/25.jpg\n","/content/drive/MyDrive/Colab Notebooks/Text-Mining/safe_images/anne_quotes/16.jpg\n","Generateing HTML table started ...\n","Collected 31 images ...\n","HTML table saved in: /content/drive/MyDrive/Colab Notebooks/Text-Mining/README.MD\n","All tasks done at 11:41:51\n"]}],"source":["# Toggles Safe mode and Automatic Made\n","# Prompt user to select crawling mode\n","mode =  toggle_safe_mode()\n","# Automatic: in this mode, the program automatically explores the web and collects \n","# images up to a maximum number specified in the 'Initalization' cell.\n","# in this mode,unethical images may also be discovered.\n","# Safe: This is a controlled mode, in which 35 images that have already been collected are processed.\n","\n","n = datetime.datetime.now()\n","\n","print (\"Process started at %s:%s:%s\" % (n.hour, n.minute, n.second))\n","\n","print(\"Mode: \",mode)\n","\n","print(\"Max number:\", max_number)\n","\n","print(\"Threshold:\",threshold)\n","\n","if mode not in [\"Y\",\"y\",\"N\",\"n\"]:\n","      print(\"Terminate by wrong keywords!\")\n","      exit()\n","\n","workspace = os.getcwd()\n","# Calling os.getcwd() has the following results:\n","# In google colab it returns '/content', This is the root directory.\n","# In GitHub codespace returns '/workspaces/codespaces-jupyter' \n","# but this is the work directory.\n","\n","# In GitHub codespace, our work directory is '/workspaces/codespaces-jupyter'\n","work_directory = workspace +\"\"\n","\n","# ENABLE THIS LINE IN 'Google Colab' WORKSPACE\n","# DISABLE THIS LINE IN GitHub' CODESPACE\n","# In google colab, our work directory is '/content/drive/MyDrive/Colab Notebooks/Text-Mining'\n","work_directory = workspace + \"/drive/MyDrive/Colab Notebooks/Text-Mining\"\n","\n","# When workspace and work directory was specified ...\n","font_path = work_directory+\"/fonts\"\n","anne_book_path = work_directory+\"/anne_books\"\n","# Directory to generate wallpapers from crawled images\n","output_path = work_directory+\"/generated_images\"\n","\n","if not os.path.exists(output_path): os.makedirs(output_path)\n","else: remove_folder_content(output_path)\n","\n","# Safe crawling is requested\n","if mode in [\"Y\",\"y\"]:\n","   print(\"Safe mode selected.\")\n","   # Defualt 'anne_quote' and 'wallpapers' folders located here.\n","   quote_path = work_directory + \"/safe_images\"\n","# Automatic crawling is requested, In automatic mode:\n","# In the first step, the images of Anne Shirley are searched\n","# In the second name step, the folder of the found images is renamed to the folder of 'anne_quotes.\n","# In the third step, wallpaper images are searched\n","# In the fourth step, the name of the folder of the found images is changed to that of 'wallpapers'.\n","elif mode in [\"N\",\"n\"]:\n","   print(\"Automatic mode selected.\")\n","   quote_path = workspace\n","   # When 'anne qoutes' crawled,by defualt,crawler creates 'images' folder in root directory\n","   # to manage root diregtory, we store it in 'workspace' varible.\n","   # Clear content of 'images' folder\n","   if os.path.exists(quote_path+\"/images\"):\n","      #remove_folder_content(workspace+\"/images\")\n","      # deletes the directory and all its contents.\n","      shutil.rmtree(quote_path +\"/images\")\n","   \n","   # Step 1: Crawling starts\n","   bing_crawler = bing.BingImageCrawler()\n","   bing_crawler.crawl(keyword = anne_keyword, offset = 0, max_num = max_number, min_size = None, max_size = None)\n","   #by default the crawler creates a folder as 'images' in root directory\n","   # and stores crawled images in, we rename this folder to 'anne_quotes'\n","   # becuase we want crawl wallpapers again.\n","    \n","   # Step 2: Rename crawled 'images' folder to 'anne_quotes'\n","   if os.path.exists(quote_path +\"/anne_quotes\"):\n","      #remove_folder_content(workspace +\"/anne_quotes\")\n","      shutil.rmtree(quote_path +\"/anne_quotes\")\n","\n","   os.rename(quote_path+\"/images\",quote_path + \"/anne_quotes\")\n","   \n","   # Step 3: Wallpapers Crawling starts\n","   #google_crawler = google.GoogleImageCrawler()\n","   bing_crawler.crawl(keyword = wallpaper_keyword, offset = 0, max_num = max_number, min_size = None, max_size = None)\n","   #by default the crawler creates a folder as 'images' in root directory\n","   # and stores crawled images in, we rename this folder to 'wallpapers'\n","   \n","   # Step 4: Rename crawled 'images' folder to 'wallpapers'\n","   if os.path.exists(quote_path +\"/wallpapers\"):\n","      shutil.rmtree(quote_path +\"/wallpapers\")\n","   \n","   if os.path.exists(quote_path+\"/images\"):\n","      os.rename(quote_path+\"/images\",quote_path +\"/wallpapers\")\n","\n","font_ = ImageFont.truetype(font = font_path +'/Vazirmatn-Regular.ttf',size= 70)\n","\n","result = auto_crawl(max_crawl= max_number , quote_path = quote_path , threshold=threshold, font = font_)\n","\n","processed = len(result[\"processed\"])\n","\n","ignored = len(result[\"ignored\"])\n","\n","undiscovered = len(result[\"undiscovered\"])\n","\n","print()\n","print(\"-------------------------------------------------------------------------------------\")\n","print(\"\\033[1mText matching threshold\\033[0m:\",threshold)\n","print(\"\\033[1mProcessed images\\033[0m:\",processed)\n","print(\"\\033[1mIgnored images\\033[0m:\",ignored)\n","\n","for item in result[\"ignored\"]:\n","    print(item[\"image_file\"]+\", obtained ratio : \",str(item[\"match\"])+\"%\")\n","\n","print(\"\\033[1mUndiscovered images\\033[0m : \",undiscovered)\n","\n","for item in result[\"undiscovered\"]:\n","       print(item)\n","\n","generate_html_table(processed_data = result[\"processed\"],directory_name = work_directory)\n","\n","n = datetime.datetime.now()\n","print (\"All tasks done at %s:%s:%s\" % (n.hour, n.minute, n.second))\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
